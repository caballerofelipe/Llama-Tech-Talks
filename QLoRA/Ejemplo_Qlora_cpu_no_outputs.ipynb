{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caballerofelipe/Llama-Tech-Talks/blob/main/Ejemplo_Qlora_cpu_no_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutado en Colab con CPU unicamente."
      ],
      "metadata": {
        "id": "ioPX1O2OHTdQ"
      },
      "id": "ioPX1O2OHTdQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalar lo que falta"
      ],
      "metadata": {
        "id": "0eiC8Cs99zVs"
      },
      "id": "0eiC8Cs99zVs"
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "brFt_LcsspZm"
      },
      "id": "brFt_LcsspZm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes trl"
      ],
      "metadata": {
        "id": "EN0klor9UB7A"
      },
      "id": "EN0klor9UB7A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "htRAqndyssb3"
      },
      "id": "htRAqndyssb3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingresar a Hugging Face"
      ],
      "metadata": {
        "id": "vx8r-qBE96le"
      },
      "id": "vx8r-qBE96le"
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar HF_TOKEN a los secretos de Colab o como variable de entorno\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "uPYmo6f3jGQV"
      },
      "id": "uPYmo6f3jGQV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importar librer√≠as"
      ],
      "metadata": {
        "id": "x2K6ARLk-D31"
      },
      "id": "x2K6ARLk-D31"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
        "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "import time\n",
        "import json\n",
        "from datasets import Dataset, load_dataset\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "YG4GH-4z-I9T"
      },
      "id": "YG4GH-4z-I9T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparaci√≥n del modelo"
      ],
      "metadata": {
        "id": "6XiNtTV3-roF"
      },
      "id": "6XiNtTV3-roF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurar la cuantizaci√≥n"
      ],
      "metadata": {
        "id": "Qyz8TFy8DGtR"
      },
      "id": "Qyz8TFy8DGtR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e0535b",
      "metadata": {
        "id": "a6e0535b"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuraci√≥n de LoRA"
      ],
      "metadata": {
        "id": "CNkJtyuNDVjN"
      },
      "id": "CNkJtyuNDVjN"
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n"
      ],
      "metadata": {
        "id": "vvEc5Aw3DSHw"
      },
      "id": "vvEc5Aw3DSHw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear el modelo con Cuantizaci√≥n"
      ],
      "metadata": {
        "id": "xArv_MSODcSU"
      },
      "id": "xArv_MSODcSU"
    },
    {
      "cell_type": "code",
      "source": [
        "# device = \"auto\"\n",
        "#device = \"cuda:0\"\n",
        "device = \"cpu\"\n",
        "\n",
        "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "model_name = model_id.split('/')[1]"
      ],
      "metadata": {
        "id": "l2A-cgbRLPfR"
      },
      "id": "l2A-cgbRLPfR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=device,\n",
        "    quantization_config=bnb_config,\n",
        "    # torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "# https://github.com/huggingface/trl/issues/3683#issuecomment-3046420837\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
        "\n",
        "#¬†Needed with Cuda, https://stackoverflow.com/a/79494162/1071459\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Aplicar LoRA\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Muestra par√°metros entrenables\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "print(f\"‚úÖ Modelo cargado en {load_time:.2f}s\")"
      ],
      "metadata": {
        "id": "UKj5ZJa4DbKR"
      },
      "id": "UKj5ZJa4DbKR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "siEUB_4ls3Pj"
      },
      "id": "siEUB_4ls3Pj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparar el Tokenizer"
      ],
      "metadata": {
        "id": "AL_ezx8l_Vx9"
      },
      "id": "AL_ezx8l_Vx9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74970683",
      "metadata": {
        "id": "74970683"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "Mk3BjW5ns6B3"
      },
      "id": "Mk3BjW5ns6B3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparar data"
      ],
      "metadata": {
        "id": "Zv0PcY6V_hJs"
      },
      "id": "Zv0PcY6V_hJs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc18fb7c",
      "metadata": {
        "id": "cc18fb7c"
      },
      "outputs": [],
      "source": [
        "# Dataset de ejemplo para chatbot de soporte t√©cnico\n",
        "sample_conversations = [\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Eres un asistente de soporte t√©cnico especializado en ayudar con problemas de software y hardware.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Mi computadora se reinicia sola, ¬øqu√© puede estar pasando?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Los reinicios autom√°ticos pueden tener varias causas. Las m√°s comunes son: 1) Sobrecalentamiento del procesador o GPU, 2) Problemas con la fuente de poder, 3) Memoria RAM defectuosa, 4) Actualizaciones autom√°ticas de Windows. Te recomiendo verificar las temperaturas con un software como HWMonitor y revisar el Visor de eventos de Windows para identificar errores espec√≠ficos.\",\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Eres un asistente de soporte t√©cnico especializado en ayudar con problemas de software y hardware.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"No puedo conectarme a WiFi desde mi laptop\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Para resolver problemas de WiFi, sigue estos pasos: 1) Verifica que el WiFi est√© activado en tu laptop (tecla Fn + tecla WiFi), 2) Reinicia el router desconect√°ndolo 30 segundos, 3) Olvida y vuelve a conectar la red WiFi, 4) Actualiza los drivers de la tarjeta de red desde el Administrador de dispositivos, 5) Ejecuta el solucionador de problemas de red de Windows. Si persiste el problema, puede ser un issue de hardware.\",\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Eres un asistente de soporte t√©cnico especializado en ayudar con problemas de software y hardware.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"Mi aplicaci√≥n se cierra inesperadamente\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Los cierres inesperados de aplicaciones pueden deberse a: 1) Falta de memoria RAM disponible, 2) Conflictos con otros programas, 3) Archivos de la aplicaci√≥n corruptos, 4) Drivers desactualizados. Soluciones: Cierra otros programas para liberar memoria, ejecuta la aplicaci√≥n como administrador, reinstala la aplicaci√≥n, actualiza drivers gr√°ficos y verifica si hay actualizaciones de la aplicaci√≥n.\",\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Eres un asistente de soporte t√©cnico especializado en ayudar con problemas de software y hardware.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"¬øC√≥mo puedo acelerar mi computadora lenta?\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Para acelerar una computadora lenta: 1) Ejecuta Liberador de espacio en disco para eliminar archivos temporales, 2) Desinstala programas que no uses, 3) Desactiva programas de inicio innecesarios en el Administrador de tareas, 4) Ejecuta un antivirus completo, 5) Considera agregar m√°s RAM si tienes menos de 8GB, 6) Reemplaza el disco duro por un SSD si a√∫n usas HDD, 7) Actualiza drivers y el sistema operativo.\",\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Eres un asistente de soporte t√©cnico especializado en ayudar con problemas de software y hardware.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"No puedo imprimir documentos\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Para resolver problemas de impresi√≥n: 1) Verifica que la impresora est√© encendida y conectada, 2) Revisa que haya papel y tinta/t√≥ner suficiente, 3) Reinicia el spooler de impresi√≥n (services.msc > Spooler de impresi√≥n > Reiniciar), 4) Actualiza o reinstala los drivers de la impresora, 5) Configura la impresora como predeterminada, 6) Ejecuta el solucionador de problemas de impresi√≥n de Windows. Si es una impresora de red, verifica la conectividad.\",\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Expandir dataset con variaciones\n",
        "expanded_conversations = []\n",
        "\n",
        "# Agregar conversaciones originales\n",
        "expanded_conversations.extend(sample_conversations)\n",
        "\n",
        "# Agregar variaciones con diferentes tonos\n",
        "for conv in sample_conversations:\n",
        "    # Versi√≥n m√°s t√©cnica\n",
        "    tech_conv = conv.copy()\n",
        "    tech_conv[\"messages\"] = conv[\"messages\"].copy()\n",
        "    tech_conv[\"messages\"][0] = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Eres un especialista t√©cnico senior con amplia experiencia en diagn√≥stico y resoluci√≥n de problemas de TI.\",\n",
        "    }\n",
        "    expanded_conversations.append(tech_conv)\n",
        "\n",
        "    # Versi√≥n m√°s amigable\n",
        "    friendly_conv = conv.copy()\n",
        "    friendly_conv[\"messages\"] = conv[\"messages\"].copy()\n",
        "    friendly_conv[\"messages\"][0] = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Eres un asistente de soporte t√©cnico amigable que explica las soluciones de manera simple y comprensible.\",\n",
        "    }\n",
        "    expanded_conversations.append(friendly_conv)\n",
        "\n",
        "# Guardar dataset\n",
        "save_path= \"sample_chatbot_data.json\"\n",
        "with open(save_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(expanded_conversations, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31aa6e8b",
      "metadata": {
        "id": "31aa6e8b"
      },
      "outputs": [],
      "source": [
        "data_path = \"sample_chatbot_data.json\"\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    conversations = json.load(f)\n",
        "len(conversations), conversations[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "641a4345",
      "metadata": {
        "id": "641a4345"
      },
      "outputs": [],
      "source": [
        "formatted_data = []\n",
        "\n",
        "for conv in conversations:\n",
        "    # Aplicar chat template\n",
        "    formatted_text = tokenizer.apply_chat_template(\n",
        "        conv[\"messages\"], tokenize=False, add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "    formatted_data.append({\"text\": formatted_text})\n",
        "\n",
        "formatted_data[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6474358d",
      "metadata": {
        "id": "6474358d"
      },
      "outputs": [],
      "source": [
        "# Crear dataset\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "# Dividir en train/validation\n",
        "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(f\"üìä Dataset dividido:\")\n",
        "print(f\"   Entrenamiento: {len(train_dataset)} ejemplos\")\n",
        "print(f\"   Validaci√≥n: {len(eval_dataset)} ejemplos\")\n",
        "\n",
        "# Mostrar ejemplo\n",
        "print(f\"\\nüìù Ejemplo de entrada formateada:\")\n",
        "# print(f\"{train_dataset[0]['text'][:200]}...\")\n",
        "print(f\"{train_dataset[0]['text']}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "hUE8-F_ZAQaR"
      },
      "id": "hUE8-F_ZAQaR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0454335",
      "metadata": {
        "id": "e0454335"
      },
      "outputs": [],
      "source": [
        "output_dir=f'{model_name}_finetuned'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to='none',  # Desactivar wandb por defecto\n",
        "    # bf16=True,\n",
        "    use_cpu=(False if device != 'cpu' else True),\n",
        "    # gradient_checkpointing=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "_4fRYVA7s-oP"
      },
      "id": "_4fRYVA7s-oP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fbf8973",
      "metadata": {
        "id": "4fbf8973"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    # model=model.to(device),\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=training_args,\n",
        "    #\n",
        "    # max_seq_length=2048,\n",
        "    # packing=False,\n",
        "    # dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configurado\")\n",
        "print('Dispositivo de entrenamiento:', trainer.args.device)\n",
        "\n",
        "# Iniciar entrenamiento\n",
        "print(\"üéØ Iniciando entrenamiento...\")\n",
        "start_time = time.time()\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚úÖ Entrenamiento completado en {training_time/60:.2f} minutos\")\n",
        "\n",
        "# Guardar modelo\n",
        "print(\"üíæ Guardando modelo...\")\n",
        "trainer.save_model(f'{model_name}_finetuned.save_model')\n",
        "tokenizer.save_pretrained(f'{model_name}_finetuned.save_pretrained')\n",
        "\n",
        "# Guardar m√©tricas\n",
        "metrics = {\n",
        "            \"training_time_minutes\": training_time / 60,\n",
        "            \"final_train_loss\": train_result.training_loss,\n",
        "            \"training_steps\": train_result.global_step,\n",
        "            \"model_name\": model_id,\n",
        "            \"peft_config\": {\n",
        "                \"r\": peft_config.r,\n",
        "                \"lora_alpha\": peft_config.lora_alpha,\n",
        "                \"lora_dropout\": peft_config.lora_dropout,\n",
        "            },\n",
        "            \"training_args\": {\n",
        "                \"num_epochs\": training_args.num_train_epochs,\n",
        "                \"batch_size\": training_args.per_device_train_batch_size,\n",
        "                \"learning_rate\": training_args.learning_rate,\n",
        "            },\n",
        "        }\n",
        "metrics\n",
        "\n",
        "with open(f\"{output_dir}/training_metrics.json\", 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(f\"üéâ Modelo fine-tuned guardado en: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "iJ9fXgJdtDA4"
      },
      "id": "iJ9fXgJdtDA4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resulados de prueba"
      ],
      "metadata": {
        "id": "-kzzKcFWF1EH"
      },
      "id": "-kzzKcFWF1EH"
    },
    {
      "cell_type": "code",
      "source": [
        "def model_generate_response(messages):\n",
        "    # Aplicar chat template\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenizar\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
        "    # print(inputs)\n",
        "\n",
        "    # Generar\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.1,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decodificar\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1] :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "-yCAmjvMHjqI"
      },
      "id": "-yCAmjvMHjqI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7822f65b",
      "metadata": {
        "id": "7822f65b"
      },
      "outputs": [],
      "source": [
        "# if test_prompts is None:\n",
        "test_prompts = [\n",
        "    \"Mi computadora no enciende, ¬øqu√© puedo hacer?\",\n",
        "    \"¬øC√≥mo puedo recuperar archivos borrados accidentalmente?\",\n",
        "    \"Mi internet va muy lento, ¬øc√≥mo lo soluciono?\",\n",
        "    \"La pantalla de mi laptop est√° parpadeando\",\n",
        "    \"No puedo instalar un programa, me da error\",\n",
        "]\n",
        "\n",
        "print(f\"üß™ Evaluando con {len(test_prompts)} prompts de prueba...\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nüîç Test {i}/{len(test_prompts)}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    # Crear conversaci√≥n de prueba\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Eres un asistente de soporte t√©cnico especializado en ayudar con problemas de software y hardware.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    # Generar respuesta\n",
        "    response = model_generate_response(messages)\n",
        "\n",
        "    print(f\"Respuesta: {response}\")\n",
        "\n",
        "    results.append({\"prompt\": prompt, \"response\": response, \"length\": len(response.split())})\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# An√°lisis de resultados\n",
        "avg_length = sum(r[\"length\"] for r in results) / len(results)\n",
        "print(f\"\\nüìä Estad√≠sticas de evaluaci√≥n:\")\n",
        "print(f\"   Longitud promedio respuesta: {avg_length:.1f} palabras\")\n",
        "print(f\"   Respuestas generadas: {len(results)}\")\n",
        "\n",
        "# Guardar resultados\n",
        "with open(f\"{output_dir}/evaluation_results.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Resultados guardados en: {output_dir}/evaluation_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üì• Cargando modelo base...\")\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if base_tokenizer.pad_token is None:\n",
        "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo base cargado\")"
      ],
      "metadata": {
        "id": "dabefd45"
      },
      "id": "dabefd45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def base_model_generate_response(messages):\n",
        "    # Aplicar chat template\n",
        "    prompt = base_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenizar\n",
        "    inputs = base_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
        "    # print(inputs)\n",
        "\n",
        "    # Generar\n",
        "    with torch.no_grad():\n",
        "        outputs = base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.1,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=base_tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decodificar\n",
        "    response = base_tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1] :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "ttplwxtwIctB"
      },
      "id": "ttplwxtwIctB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparar con modelo base\n",
        "test_prompts = [\n",
        "    \"Mi computadora no enciende\",\n",
        "    \"No puedo conectarme a internet\",\n",
        "    \"La aplicaci√≥n se cierra sola\",\n",
        "]\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\nüîç Comparaci√≥n {i}/{len(test_prompts)}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Eres un asistente de soporte t√©cnico especializado en ayudar con problemas de software y hardware.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    base_response = base_model_generate_response(messages)\n",
        "    finetuned_response = model_generate_response(messages)\n",
        "\n",
        "    print(f\"ü§ñ Modelo Base: {base_response}\")\n",
        "    print(f\"üéØ Fine-tuned: {finetuned_response}\")\n",
        "\n",
        "    comparison_results.append(\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"base_response\": base_response,\n",
        "            \"finetuned_response\": finetuned_response,\n",
        "            \"base_length\": len(base_response.split()),\n",
        "            \"finetuned_length\": len(finetuned_response.split()),\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "Yeq63z1IJ0RV"
      },
      "id": "Yeq63z1IJ0RV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "8svockDgtGVh"
      },
      "id": "8svockDgtGVh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sitios √ötiles:\n",
        "- https://github.com/huggingface/peft"
      ],
      "metadata": {
        "id": "9ZwmbD6M9Xuc"
      },
      "id": "9ZwmbD6M9Xuc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}