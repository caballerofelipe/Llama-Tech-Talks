{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caballerofelipe/Llama-Tech-Talks/blob/main/Ejemplo_Qlora_gpu_A100_no_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutado en Colab con GPU A100."
      ],
      "metadata": {
        "id": "F0Ane4UTHnJo"
      },
      "id": "F0Ane4UTHnJo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalar lo que falta"
      ],
      "metadata": {
        "id": "0eiC8Cs99zVs"
      },
      "id": "0eiC8Cs99zVs"
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "brFt_LcsspZm"
      },
      "id": "brFt_LcsspZm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes trl"
      ],
      "metadata": {
        "id": "EN0klor9UB7A"
      },
      "id": "EN0klor9UB7A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "htRAqndyssb3"
      },
      "id": "htRAqndyssb3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingresar a Hugging Face"
      ],
      "metadata": {
        "id": "vx8r-qBE96le"
      },
      "id": "vx8r-qBE96le"
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar HF_TOKEN a los secretos de Colab o como variable de entorno\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "uPYmo6f3jGQV"
      },
      "id": "uPYmo6f3jGQV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importar librerías"
      ],
      "metadata": {
        "id": "x2K6ARLk-D31"
      },
      "id": "x2K6ARLk-D31"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
        "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "import time\n",
        "import json\n",
        "from datasets import Dataset, load_dataset\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "YG4GH-4z-I9T"
      },
      "id": "YG4GH-4z-I9T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparación del modelo"
      ],
      "metadata": {
        "id": "6XiNtTV3-roF"
      },
      "id": "6XiNtTV3-roF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurar la cuantización"
      ],
      "metadata": {
        "id": "Qyz8TFy8DGtR"
      },
      "id": "Qyz8TFy8DGtR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e0535b",
      "metadata": {
        "id": "a6e0535b"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración de LoRA"
      ],
      "metadata": {
        "id": "CNkJtyuNDVjN"
      },
      "id": "CNkJtyuNDVjN"
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n"
      ],
      "metadata": {
        "id": "vvEc5Aw3DSHw"
      },
      "id": "vvEc5Aw3DSHw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear el modelo con Cuantización"
      ],
      "metadata": {
        "id": "xArv_MSODcSU"
      },
      "id": "xArv_MSODcSU"
    },
    {
      "cell_type": "code",
      "source": [
        "# device = \"auto\"\n",
        "device = \"cuda:0\"\n",
        "# device = \"cpu\"\n",
        "\n",
        "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "model_name = model_id.split('/')[1]"
      ],
      "metadata": {
        "id": "l2A-cgbRLPfR"
      },
      "id": "l2A-cgbRLPfR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=device,\n",
        "    quantization_config=bnb_config,\n",
        "    # torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "# https://github.com/huggingface/trl/issues/3683#issuecomment-3046420837\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
        "\n",
        "# Needed with Cuda, https://stackoverflow.com/a/79494162/1071459\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Aplicar LoRA\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Muestra parámetros entrenables\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "print(f\"✅ Modelo cargado en {load_time:.2f}s\")"
      ],
      "metadata": {
        "id": "UKj5ZJa4DbKR"
      },
      "id": "UKj5ZJa4DbKR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "siEUB_4ls3Pj"
      },
      "id": "siEUB_4ls3Pj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparar el Tokenizer"
      ],
      "metadata": {
        "id": "AL_ezx8l_Vx9"
      },
      "id": "AL_ezx8l_Vx9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74970683",
      "metadata": {
        "id": "74970683"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'\n",
        "\n",
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "Mk3BjW5ns6B3"
      },
      "id": "Mk3BjW5ns6B3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparar data"
      ],
      "metadata": {
        "id": "Zv0PcY6V_hJs"
      },
      "id": "Zv0PcY6V_hJs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc18fb7c",
      "metadata": {
        "id": "cc18fb7c"
      },
      "outputs": [],
      "source": [
        "# Dataset de ejemplo para chatbot de soporte técnico\n",
        "sample_conversations = [\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Eres un asistente de soporte técnico especializado en ayudar con problemas de software y hardware.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Mi computadora se reinicia sola, ¿qué puede estar pasando?\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Los reinicios automáticos pueden tener varias causas. Las más comunes son: 1) Sobrecalentamiento del procesador o GPU, 2) Problemas con la fuente de poder, 3) Memoria RAM defectuosa, 4) Actualizaciones automáticas de Windows. Te recomiendo verificar las temperaturas con un software como HWMonitor y revisar el Visor de eventos de Windows para identificar errores específicos.\",\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Eres un asistente de soporte técnico especializado en ayudar con problemas de software y hardware.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"No puedo conectarme a WiFi desde mi laptop\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Para resolver problemas de WiFi, sigue estos pasos: 1) Verifica que el WiFi esté activado en tu laptop (tecla Fn + tecla WiFi), 2) Reinicia el router desconectándolo 30 segundos, 3) Olvida y vuelve a conectar la red WiFi, 4) Actualiza los drivers de la tarjeta de red desde el Administrador de dispositivos, 5) Ejecuta el solucionador de problemas de red de Windows. Si persiste el problema, puede ser un issue de hardware.\",\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Eres un asistente de soporte técnico especializado en ayudar con problemas de software y hardware.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"Mi aplicación se cierra inesperadamente\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Los cierres inesperados de aplicaciones pueden deberse a: 1) Falta de memoria RAM disponible, 2) Conflictos con otros programas, 3) Archivos de la aplicación corruptos, 4) Drivers desactualizados. Soluciones: Cierra otros programas para liberar memoria, ejecuta la aplicación como administrador, reinstala la aplicación, actualiza drivers gráficos y verifica si hay actualizaciones de la aplicación.\",\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Eres un asistente de soporte técnico especializado en ayudar con problemas de software y hardware.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"¿Cómo puedo acelerar mi computadora lenta?\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Para acelerar una computadora lenta: 1) Ejecuta Liberador de espacio en disco para eliminar archivos temporales, 2) Desinstala programas que no uses, 3) Desactiva programas de inicio innecesarios en el Administrador de tareas, 4) Ejecuta un antivirus completo, 5) Considera agregar más RAM si tienes menos de 8GB, 6) Reemplaza el disco duro por un SSD si aún usas HDD, 7) Actualiza drivers y el sistema operativo.\",\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Eres un asistente de soporte técnico especializado en ayudar con problemas de software y hardware.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"No puedo imprimir documentos\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Para resolver problemas de impresión: 1) Verifica que la impresora esté encendida y conectada, 2) Revisa que haya papel y tinta/tóner suficiente, 3) Reinicia el spooler de impresión (services.msc > Spooler de impresión > Reiniciar), 4) Actualiza o reinstala los drivers de la impresora, 5) Configura la impresora como predeterminada, 6) Ejecuta el solucionador de problemas de impresión de Windows. Si es una impresora de red, verifica la conectividad.\",\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Expandir dataset con variaciones\n",
        "expanded_conversations = []\n",
        "\n",
        "# Agregar conversaciones originales\n",
        "expanded_conversations.extend(sample_conversations)\n",
        "\n",
        "# Agregar variaciones con diferentes tonos\n",
        "for conv in sample_conversations:\n",
        "    # Versión más técnica\n",
        "    tech_conv = conv.copy()\n",
        "    tech_conv[\"messages\"] = conv[\"messages\"].copy()\n",
        "    tech_conv[\"messages\"][0] = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Eres un especialista técnico senior con amplia experiencia en diagnóstico y resolución de problemas de TI.\",\n",
        "    }\n",
        "    expanded_conversations.append(tech_conv)\n",
        "\n",
        "    # Versión más amigable\n",
        "    friendly_conv = conv.copy()\n",
        "    friendly_conv[\"messages\"] = conv[\"messages\"].copy()\n",
        "    friendly_conv[\"messages\"][0] = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Eres un asistente de soporte técnico amigable que explica las soluciones de manera simple y comprensible.\",\n",
        "    }\n",
        "    expanded_conversations.append(friendly_conv)\n",
        "\n",
        "# Guardar dataset\n",
        "save_path= \"sample_chatbot_data.json\"\n",
        "with open(save_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(expanded_conversations, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31aa6e8b",
      "metadata": {
        "id": "31aa6e8b"
      },
      "outputs": [],
      "source": [
        "data_path = \"sample_chatbot_data.json\"\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    conversations = json.load(f)\n",
        "len(conversations), conversations[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "641a4345",
      "metadata": {
        "id": "641a4345"
      },
      "outputs": [],
      "source": [
        "formatted_data = []\n",
        "\n",
        "for conv in conversations:\n",
        "    # Aplicar chat template\n",
        "    formatted_text = tokenizer.apply_chat_template(\n",
        "        conv[\"messages\"], tokenize=False, add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "    formatted_data.append({\"text\": formatted_text})\n",
        "\n",
        "formatted_data[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6474358d",
      "metadata": {
        "id": "6474358d"
      },
      "outputs": [],
      "source": [
        "# Crear dataset\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "# Dividir en train/validation\n",
        "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(f\"📊 Dataset dividido:\")\n",
        "print(f\"   Entrenamiento: {len(train_dataset)} ejemplos\")\n",
        "print(f\"   Validación: {len(eval_dataset)} ejemplos\")\n",
        "\n",
        "# Mostrar ejemplo\n",
        "print(f\"\\n📝 Ejemplo de entrada formateada:\")\n",
        "# print(f\"{train_dataset[0]['text'][:200]}...\")\n",
        "print(f\"{train_dataset[0]['text']}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "hUE8-F_ZAQaR"
      },
      "id": "hUE8-F_ZAQaR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0454335",
      "metadata": {
        "id": "e0454335"
      },
      "outputs": [],
      "source": [
        "output_dir=f'{model_name}_finetuned'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to='none',  # Desactivar wandb por defecto\n",
        "    # bf16=True,\n",
        "    use_cpu=(False if device != 'cpu' else True),\n",
        "    # gradient_checkpointing=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "_4fRYVA7s-oP"
      },
      "id": "_4fRYVA7s-oP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fbf8973",
      "metadata": {
        "id": "4fbf8973"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    # model=model.to(device),\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=training_args,\n",
        "    #\n",
        "    # max_seq_length=2048,\n",
        "    # packing=False,\n",
        "    # dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "print(\"✅ Trainer configurado\")\n",
        "print('Dispositivo de entrenamiento:', trainer.args.device)\n",
        "\n",
        "# Iniciar entrenamiento\n",
        "print(\"🎯 Iniciando entrenamiento...\")\n",
        "start_time = time.time()\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"✅ Entrenamiento completado en {training_time/60:.2f} minutos\")\n",
        "\n",
        "# Guardar modelo\n",
        "print(\"💾 Guardando modelo...\")\n",
        "trainer.save_model(f'{model_name}_finetuned.save_model')\n",
        "tokenizer.save_pretrained(f'{model_name}_finetuned.save_pretrained')\n",
        "\n",
        "# Guardar métricas\n",
        "metrics = {\n",
        "            \"training_time_minutes\": training_time / 60,\n",
        "            \"final_train_loss\": train_result.training_loss,\n",
        "            \"training_steps\": train_result.global_step,\n",
        "            \"model_name\": model_id,\n",
        "            \"peft_config\": {\n",
        "                \"r\": peft_config.r,\n",
        "                \"lora_alpha\": peft_config.lora_alpha,\n",
        "                \"lora_dropout\": peft_config.lora_dropout,\n",
        "            },\n",
        "            \"training_args\": {\n",
        "                \"num_epochs\": training_args.num_train_epochs,\n",
        "                \"batch_size\": training_args.per_device_train_batch_size,\n",
        "                \"learning_rate\": training_args.learning_rate,\n",
        "            },\n",
        "        }\n",
        "metrics\n",
        "\n",
        "with open(f\"{output_dir}/training_metrics.json\", 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(f\"🎉 Modelo fine-tuned guardado en: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "iJ9fXgJdtDA4"
      },
      "id": "iJ9fXgJdtDA4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resulados de prueba"
      ],
      "metadata": {
        "id": "-kzzKcFWF1EH"
      },
      "id": "-kzzKcFWF1EH"
    },
    {
      "cell_type": "code",
      "source": [
        "def model_generate_response(messages):\n",
        "    # Aplicar chat template\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenizar\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
        "    # print(inputs)\n",
        "\n",
        "    # Generar\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.1,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decodificar\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1] :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "-yCAmjvMHjqI"
      },
      "id": "-yCAmjvMHjqI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7822f65b",
      "metadata": {
        "id": "7822f65b"
      },
      "outputs": [],
      "source": [
        "# if test_prompts is None:\n",
        "test_prompts = [\n",
        "    \"Mi computadora no enciende, ¿qué puedo hacer?\",\n",
        "    \"¿Cómo puedo recuperar archivos borrados accidentalmente?\",\n",
        "    \"Mi internet va muy lento, ¿cómo lo soluciono?\",\n",
        "    \"La pantalla de mi laptop está parpadeando\",\n",
        "    \"No puedo instalar un programa, me da error\",\n",
        "]\n",
        "\n",
        "print(f\"🧪 Evaluando con {len(test_prompts)} prompts de prueba...\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n🔍 Test {i}/{len(test_prompts)}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    # Crear conversación de prueba\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Eres un asistente de soporte técnico especializado en ayudar con problemas de software y hardware.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    # Generar respuesta\n",
        "    response = model_generate_response(messages)\n",
        "\n",
        "    print(f\"Respuesta: {response}\")\n",
        "\n",
        "    results.append({\"prompt\": prompt, \"response\": response, \"length\": len(response.split())})\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Análisis de resultados\n",
        "avg_length = sum(r[\"length\"] for r in results) / len(results)\n",
        "print(f\"\\n📊 Estadísticas de evaluación:\")\n",
        "print(f\"   Longitud promedio respuesta: {avg_length:.1f} palabras\")\n",
        "print(f\"   Respuestas generadas: {len(results)}\")\n",
        "\n",
        "# Guardar resultados\n",
        "with open(f\"{output_dir}/evaluation_results.json\", 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✅ Resultados guardados en: {output_dir}/evaluation_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"📥 Cargando modelo base...\")\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if base_tokenizer.pad_token is None:\n",
        "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device,\n",
        ")\n",
        "\n",
        "print(\"✅ Modelo base cargado\")"
      ],
      "metadata": {
        "id": "dabefd45"
      },
      "id": "dabefd45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def base_model_generate_response(messages):\n",
        "    # Aplicar chat template\n",
        "    prompt = base_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Tokenizar\n",
        "    inputs = base_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
        "    # print(inputs)\n",
        "\n",
        "    # Generar\n",
        "    with torch.no_grad():\n",
        "        outputs = base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.1,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=base_tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decodificar\n",
        "    response = base_tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1] :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "ttplwxtwIctB"
      },
      "id": "ttplwxtwIctB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparar con modelo base\n",
        "test_prompts = [\n",
        "    \"Mi computadora no enciende\",\n",
        "    \"No puedo conectarme a internet\",\n",
        "    \"La aplicación se cierra sola\",\n",
        "]\n",
        "\n",
        "comparison_results = []\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n🔍 Comparación {i}/{len(test_prompts)}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Eres un asistente de soporte técnico especializado en ayudar con problemas de software y hardware.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    base_response = base_model_generate_response(messages)\n",
        "    finetuned_response = model_generate_response(messages)\n",
        "\n",
        "    print(f\"🤖 Modelo Base: {base_response}\")\n",
        "    print(f\"🎯 Fine-tuned: {finetuned_response}\")\n",
        "\n",
        "    comparison_results.append(\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"base_response\": base_response,\n",
        "            \"finetuned_response\": finetuned_response,\n",
        "            \"base_length\": len(base_response.split()),\n",
        "            \"finetuned_length\": len(finetuned_response.split()),\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "Yeq63z1IJ0RV"
      },
      "id": "Yeq63z1IJ0RV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "id": "8svockDgtGVh"
      },
      "id": "8svockDgtGVh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sitios Útiles:\n",
        "- https://github.com/huggingface/peft"
      ],
      "metadata": {
        "id": "9ZwmbD6M9Xuc"
      },
      "id": "9ZwmbD6M9Xuc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}