# ğŸš€ MÃ³dulo 3: Deployment y Consumo de Llama 3.1 con Ollama

## ğŸ“‹ DescripciÃ³n

Este mÃ³dulo te enseÃ±a a **desplegar y consumir modelos Llama 3.1** usando **Ollama**, una herramienta que permite ejecutar LLMs localmente de forma eficiente. AprenderÃ¡s desde la instalaciÃ³n hasta el desarrollo de aplicaciones que consumen el modelo a travÃ©s de API REST.

## ğŸ“ Contenido del MÃ³dulo

```
codigo/modulo3/
â”œâ”€â”€ README.md                        # Este archivo
â””â”€â”€ laboratorio-modulo-n3.ipynb     # Laboratorio completo de Ollama
```

## ğŸ¯ Objetivos de Aprendizaje

Al completar este mÃ³dulo, dominarÃ¡s:

1. **ğŸ› ï¸ InstalaciÃ³n y configuraciÃ³n** de Ollama en Windows/Mac
2. **âš™ï¸ ConfiguraciÃ³n del entorno** de desarrollo con VS Code
3. **ğŸ¦™ Descarga y ejecuciÃ³n** de modelos Llama 3.1 localmente
4. **ğŸŒ Consumo de modelos** a travÃ©s de API REST
5. **ğŸ”§ ConfiguraciÃ³n de parÃ¡metros** de generaciÃ³n
6. **ğŸ“¡ Streaming de respuestas** en tiempo real
7. **ğŸ Desarrollo de aplicaciones** Python que integran LLMs
8. **ğŸŒ CreaciÃ³n de APIs web** con Flask

## ğŸ› ï¸ Herramientas y TecnologÃ­as

### **ğŸ”§ Software Requerido:**
- **Visual Studio Code** - Editor de cÃ³digo principal
- **Python 3.10+** - Lenguaje de programaciÃ³n
- **Ollama** - Runtime para modelos LLM locales
- **Git** (opcional) - Control de versiones

### **ğŸ“¦ LibrerÃ­as Python:**
- **requests** - Cliente HTTP para API REST
- **jupyter** - Notebooks interactivos
- **flask** (opcional) - Framework web
- **json** - Manejo de datos JSON

## ğŸ““ Laboratorio PrÃ¡ctico: `laboratorio-modulo-n3.ipynb`

### **ğŸš€ Estructura del Laboratorio:**

#### **0. ğŸ› ï¸ InstalaciÃ³n de Herramientas**
- **Visual Studio Code**: Editor principal
- **Python 3.10+**: Entorno de desarrollo
- **Extensiones VS Code**: Python y Jupyter

#### **1. ğŸ”§ PreparaciÃ³n del Entorno**
- **Entorno virtual**: Aislamiento de dependencias
- **InstalaciÃ³n de librerÃ­as**: requests, jupyter
- **ConfiguraciÃ³n de VS Code**: Terminal integrada

#### **2. ğŸ“¥ InstalaciÃ³n de Ollama**
- **Windows**: Instalador oficial + WSL2
- **Mac**: Homebrew o instalador oficial
- **VerificaciÃ³n**: Comando `ollama --version`

#### **3. ğŸ¦™ Descarga y EjecuciÃ³n de Llama 3.1**
- **Descarga del modelo**: `ollama pull llama3`
- **Prueba desde terminal**: Comandos bÃ¡sicos
- **VerificaciÃ³n de funcionamiento**: Respuestas simples

#### **4. ğŸŒ Consumo via API REST**
- **ConfiguraciÃ³n de cliente**: requests HTTP
- **Llamadas bÃ¡sicas**: GeneraciÃ³n de texto
- **ParÃ¡metros de configuraciÃ³n**: temperature, top_p
- **Manejo de respuestas**: JSON parsing

#### **5. ğŸ“¡ Streaming de Respuestas**
- **Respuestas en tiempo real**: stream=True
- **Procesamiento incremental**: iter_lines()
- **Experiencia de usuario**: Feedback inmediato

#### **6. ğŸ›¡ï¸ Manejo de Errores**
- **Errores de conexiÃ³n**: Ollama no disponible
- **Timeouts**: Respuestas lentas
- **Manejo robusto**: try/except patterns

#### **7. ğŸŒ Servidor Web con Flask**
- **API REST personalizada**: Endpoints propios
- **IntegraciÃ³n con Ollama**: Proxy de requests
- **Desarrollo web**: Aplicaciones completas

## ğŸš€ GuÃ­a de InstalaciÃ³n Paso a Paso

### **1. ğŸ–¥ï¸ InstalaciÃ³n de Visual Studio Code**

#### **Windows/Mac:**
1. Descarga desde [https://code.visualstudio.com/](https://code.visualstudio.com/)
2. Ejecuta el instalador
3. Instala extensiones requeridas:
   - **Python** (Microsoft)
   - **Jupyter** (Microsoft)

### **2. ğŸ InstalaciÃ³n de Python**

#### **Windows:**
1. Descarga desde [https://www.python.org/downloads/](https://www.python.org/downloads/)
2. **IMPORTANTE**: Marca "Add Python to PATH"
3. Verifica: `python --version`

#### **Mac:**
```bash
# OpciÃ³n 1: Instalador oficial
# Descarga desde python.org

# OpciÃ³n 2: Homebrew
brew install python
```

### **3. ğŸ¦™ InstalaciÃ³n de Ollama**

#### **Windows:**
1. Descarga desde [https://ollama.ai/download](https://ollama.ai/download)
2. Ejecuta el instalador
3. WSL2 se instala automÃ¡ticamente si es necesario
4. Verifica: `ollama --version`

#### **Mac:**
```bash
# OpciÃ³n 1: Homebrew (recomendado)
brew install ollama

# OpciÃ³n 2: Instalador oficial
# Descarga desde ollama.ai/download
```

### **4. ğŸ”§ ConfiguraciÃ³n del Entorno**

#### **Crear entorno virtual:**
```bash
# Windows
python -m venv venv
.\venv\Scripts\activate

# Mac/Linux
python3 -m venv venv
source venv/bin/activate
```

#### **Instalar dependencias:**
```bash
pip install jupyter requests flask
```

## ğŸ¦™ Uso de Ollama

### **ğŸ“¥ Descarga de Modelos**

```bash
# Llama 3.1 (recomendado)
ollama pull llama3

# Otros modelos disponibles
ollama pull llama3:70b      # VersiÃ³n grande
ollama pull codellama       # Especializado en cÃ³digo
ollama pull mistral         # Alternativa rÃ¡pida
```

### **ğŸ” Comandos BÃ¡sicos**

```bash
# Listar modelos instalados
ollama list

# Ejecutar modelo interactivo
ollama run llama3

# Ejecutar con prompt directo
ollama run llama3 "Explica quÃ© es la IA"

# Iniciar servidor (si no estÃ¡ corriendo)
ollama serve
```

### **âš™ï¸ ConfiguraciÃ³n del Servidor**

```bash
# Puerto por defecto: 11434
# Host por defecto: localhost
# URL completa: http://localhost:11434
```

## ğŸŒ API REST de Ollama

### **ğŸ“¡ Endpoint Principal**

```
POST http://localhost:11434/api/generate
```

### **ğŸ“‹ ParÃ¡metros de Request**

```json
{
  "model": "llama3",
  "prompt": "Tu pregunta aquÃ­",
  "stream": false,
  "temperature": 0.7,
  "top_p": 0.9,
  "max_tokens": 100
}
```

### **ğŸ“Š ParÃ¡metros de ConfiguraciÃ³n**

| **ParÃ¡metro** | **Rango** | **DescripciÃ³n** | **Uso Recomendado** |
|---------------|-----------|-----------------|---------------------|
| `temperature` | 0.0-2.0 | Creatividad/Aleatoriedad | 0.3 (conservador), 0.7 (balanceado), 1.0 (creativo) |
| `top_p` | 0.0-1.0 | Diversidad de tokens | 0.9 (recomendado) |
| `stream` | true/false | Respuesta en tiempo real | true (UX mejor), false (simple) |
| `max_tokens` | 1-âˆ | Longitud mÃ¡xima | 100-500 (tÃ­pico) |

### **ğŸ Ejemplo de CÃ³digo Python**

```python
import requests
import json

OLLAMA_HOST = "http://localhost:11434"

def consultar_llama(prompt, temperature=0.7):
    payload = {
        "model": "llama3",
        "prompt": prompt,
        "temperature": temperature,
        "stream": False
    }
    
    try:
        response = requests.post(
            f"{OLLAMA_HOST}/api/generate", 
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        return response.json().get("response", "")
    
    except requests.exceptions.ConnectionError:
        return "Error: Ollama no estÃ¡ corriendo"
    except Exception as e:
        return f"Error: {e}"

# Uso
respuesta = consultar_llama("Â¿QuÃ© es machine learning?")
print(respuesta)
```

## ğŸ“¡ Streaming de Respuestas

### **ğŸ”„ ImplementaciÃ³n de Streaming**

```python
def consultar_streaming(prompt):
    payload = {
        "model": "llama3",
        "prompt": prompt,
        "stream": True
    }
    
    response = requests.post(
        f"{OLLAMA_HOST}/api/generate", 
        json=payload, 
        stream=True
    )
    
    print("Respuesta: ", end="", flush=True)
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            print(chunk.get("response", ""), end="", flush=True)
            if chunk.get("done"):
                print("\n--- Completado ---")
                break
```

### **âœ¨ Ventajas del Streaming**

- **âš¡ Respuesta inmediata**: Usuario ve progreso
- **ğŸ¯ Mejor UX**: SensaciÃ³n de velocidad
- **ğŸ”„ Interactividad**: Posibilidad de cancelar
- **ğŸ“± Apps en tiempo real**: Chat interfaces

## ğŸŒ Desarrollo de APIs Web

### **ğŸ Servidor Flask BÃ¡sico**

```python
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)
OLLAMA_HOST = "http://localhost:11434"

@app.route('/chat', methods=['POST'])
def chat():
    datos = request.json
    prompt = datos.get("prompt", "")
    
    payload = {
        "model": "llama3",
        "prompt": prompt,
        "stream": False
    }
    
    try:
        resp = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload)
        return jsonify(resp.json())
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(port=5000, debug=True)
```

### **ğŸŒ Frontend HTML Simple**

```html
<!DOCTYPE html>
<html>
<head>
    <title>Chat con Llama 3.1</title>
</head>
<body>
    <div id="chat"></div>
    <input type="text" id="prompt" placeholder="Escribe tu pregunta...">
    <button onclick="enviarPregunta()">Enviar</button>
    
    <script>
        async function enviarPregunta() {
            const prompt = document.getElementById('prompt').value;
            const response = await fetch('/chat', {
                method: 'POST',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({prompt: prompt})
            });
            const data = await response.json();
            document.getElementById('chat').innerHTML += 
                `<p><strong>TÃº:</strong> ${prompt}</p>
                 <p><strong>Llama:</strong> ${data.response}</p>`;
        }
    </script>
</body>
</html>
```

## ğŸš¨ SoluciÃ³n de Problemas

### **âŒ "Ollama no estÃ¡ corriendo"**
**Problema**: Error de conexiÃ³n a localhost:11434
**Soluciones**:
1. Ejecutar `ollama serve` en terminal
2. Verificar que el puerto 11434 estÃ© libre
3. Reiniciar Ollama: `ollama stop` â†’ `ollama serve`

### **âŒ "Modelo no encontrado"**
**Problema**: Model 'llama3' not found
**Soluciones**:
1. Descargar modelo: `ollama pull llama3`
2. Verificar modelos: `ollama list`
3. Usar modelo alternativo disponible

### **âŒ "Respuestas muy lentas"**
**Problema**: Timeouts o respuestas tardÃ­as
**Soluciones**:
1. Reducir `max_tokens`
2. Aumentar `timeout` en requests
3. Usar modelo mÃ¡s pequeÃ±o
4. Verificar recursos del sistema (RAM/CPU)

### **âŒ "Error de memoria"**
**Problema**: Sistema sin recursos suficientes
**Soluciones**:
1. Cerrar aplicaciones innecesarias
2. Usar modelo mÃ¡s pequeÃ±o (llama3:7b)
3. Aumentar memoria virtual
4. Considerar modelo cuantizado

## ğŸ“ Ejercicios PrÃ¡cticos

### **Nivel BÃ¡sico:**
1. **InstalaciÃ³n completa**: Configura todo el entorno
2. **Primera consulta**: Haz una pregunta simple a Llama 3.1
3. **ParÃ¡metros bÃ¡sicos**: Experimenta con temperature
4. **Manejo de errores**: Implementa try/except

### **Nivel Intermedio:**
1. **Streaming**: Implementa respuestas en tiempo real
2. **API personalizada**: Crea endpoints con Flask
3. **Frontend simple**: HTML + JavaScript bÃ¡sico
4. **Configuraciones**: Compara diferentes parÃ¡metros

### **Nivel Avanzado:**
1. **Chat completo**: AplicaciÃ³n web funcional
2. **MÃºltiples modelos**: Soporte para varios LLMs
3. **AutenticaciÃ³n**: Sistema de usuarios
4. **OptimizaciÃ³n**: CachÃ© y performance

## ğŸ”— Recursos Adicionales

### **ğŸ“š DocumentaciÃ³n Oficial:**
- [Ollama Documentation](https://ollama.ai/docs)
- [Ollama API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Llama 3.1 Model Card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)

### **ğŸ› ï¸ Herramientas Complementarias:**
- [Ollama Web UI](https://github.com/open-webui/open-webui) - Interfaz web completa
- [LangChain](https://langchain.com/) - Framework para LLMs
- [Streamlit](https://streamlit.io/) - Apps rÃ¡pidas de ML

### **ğŸ¥ Videos y Tutoriales:**
- [Ollama Official Channel](https://www.youtube.com/@ollama)
- [Local LLM Setup Guides](https://www.youtube.com/results?search_query=ollama+setup)

## ğŸš€ PrÃ³ximos Pasos

DespuÃ©s de completar este mÃ³dulo:

1. **MÃ³dulo 4**: Fine-tuning y personalizaciÃ³n avanzada
2. **MÃ³dulo 5**: Aplicaciones empresariales y casos de uso
3. **MÃ³dulo 6**: OptimizaciÃ³n y despliegue en producciÃ³n

## ğŸ¤ Contribuciones

Â¿Tienes mejoras para el laboratorio?

1. Reporta problemas de instalaciÃ³n
2. Sugiere nuevos ejercicios
3. Comparte casos de uso interesantes
4. Contribuye con optimizaciones

## ğŸ“„ Licencia

Este material educativo estÃ¡ disponible bajo licencia MIT. Libre uso para educaciÃ³n y desarrollo.

---

**Â¡Despliega Llama 3.1 localmente y crea aplicaciones increÃ­bles!** ğŸ¦™ğŸš€âœ¨
