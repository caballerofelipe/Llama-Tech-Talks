# ğŸ§  MÃ³dulo 1: Fundamentos de Transformers y Mecanismo de AtenciÃ³n

## ğŸ“‹ DescripciÃ³n

Este mÃ³dulo introduce los **conceptos fundamentales** de los modelos Transformer y el **mecanismo de atenciÃ³n**, que son la base de los modelos de lenguaje modernos como GPT, BERT, y Llama.

## ğŸ“ Contenido del MÃ³dulo

```
codigo/modulo1/
â”œâ”€â”€ README.md                 # Este archivo
â””â”€â”€ demo-atencion.ipynb      # Demo interactiva del mecanismo de atenciÃ³n
```

## ğŸ¯ Objetivos de Aprendizaje

Al completar este mÃ³dulo, entenderÃ¡s:

1. **ğŸ” QuÃ© es el mecanismo de atenciÃ³n** y por quÃ© es revolucionario
2. **ğŸ§® CÃ³mo funciona matemÃ¡ticamente** la atenciÃ³n en los Transformers
3. **ğŸ‘ï¸ VisualizaciÃ³n prÃ¡ctica** de patrones de atenciÃ³n en modelos reales
4. **ğŸ”— Relaciones entre tokens** y cÃ³mo el modelo las aprende
5. **ğŸ—ï¸ Arquitectura bÃ¡sica** de los modelos Transformer

## ğŸ““ Demo Interactiva: Mecanismo de AtenciÃ³n

### **ğŸš€ `demo-atencion.ipynb`**

Un notebook interactivo que te permite **explorar visualmente** cÃ³mo funciona el mecanismo de atenciÃ³n en un modelo BERT real.

#### **âœ¨ CaracterÃ­sticas:**

- ğŸ® **Interfaz interactiva** con widgets para explorar diferentes capas y cabezas
- ğŸ“Š **VisualizaciÃ³n en tiempo real** de patrones de atenciÃ³n
- ğŸ‡ªğŸ‡¸ **Modelo en espaÃ±ol** (BERT espaÃ±ol de la Universidad de Chile)
- ğŸ¨ **GrÃ¡ficos intuitivos** que muestran conexiones entre tokens
- ğŸ”¢ **Valores numÃ©ricos** de atenciÃ³n para anÃ¡lisis detallado

#### **ğŸ›ï¸ Controles Interactivos:**

- **Layer Selector**: Explora las 12 capas del modelo BERT
- **Head Selector**: Examina las 12 cabezas de atenciÃ³n por capa
- **Token Selector**: Elige quÃ© token analizar como origen de atenciÃ³n

#### **ğŸ“ Ejemplo de AnÃ¡lisis:**

**Frase:** *"El perro que persiguiÃ³ al gato cruzÃ³ la calle."*

**Observaciones tÃ­picas:**
- **Capa 0-2**: AtenciÃ³n principalmente a tokens adyacentes
- **Capa 3-6**: Patrones sintÃ¡cticos (sujeto-verbo, artÃ­culo-sustantivo)
- **Capa 7-11**: Relaciones semÃ¡nticas complejas y dependencias largas

## ğŸ› ï¸ InstalaciÃ³n y Uso

### **Requisitos:**
```bash
pip install transformers==4.41.2
pip install ipywidgets
pip install matplotlib
pip install torch
```

### **EjecuciÃ³n:**

1. **Abrir el notebook:**
   ```bash
   jupyter notebook demo-atencion.ipynb
   ```

2. **En Kaggle/Colab:**
   - Sube el archivo `demo-atencion.ipynb`
   - Ejecuta todas las celdas secuencialmente
   - InteractÃºa con los widgets para explorar

3. **Uso local:**
   - AsegÃºrate de tener Jupyter instalado
   - Ejecuta en un entorno con las dependencias instaladas

## ğŸ” Conceptos Clave Explicados

### **1. Mecanismo de AtenciÃ³n**

El mecanismo de atenciÃ³n permite que cada token en una secuencia **"preste atenciÃ³n"** a todos los demÃ¡s tokens, determinando quÃ© informaciÃ³n es relevante para su procesamiento.

**FÃ³rmula bÃ¡sica:**
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
```

Donde:
- **Q** (Query): "Â¿QuÃ© estoy buscando?"
- **K** (Key): "Â¿QuÃ© informaciÃ³n tengo?"
- **V** (Value): "Â¿CuÃ¡l es esa informaciÃ³n?"

### **2. Multi-Head Attention**

Cada capa tiene **mÃºltiples cabezas** de atenciÃ³n que aprenden diferentes tipos de relaciones:

- **Cabeza 1**: Relaciones sintÃ¡cticas (sujeto-verbo)
- **Cabeza 2**: Dependencias semÃ¡nticas
- **Cabeza 3**: Patrones posicionales
- **...y asÃ­ sucesivamente**

### **3. Capas y EspecializaciÃ³n**

- **Capas tempranas (0-3)**: Patrones locales y sintaxis bÃ¡sica
- **Capas medias (4-8)**: Estructura gramatical y dependencias
- **Capas tardÃ­as (9-11)**: SemÃ¡ntica compleja y razonamiento

## ğŸ“Š InterpretaciÃ³n de Visualizaciones

### **ğŸ¨ Elementos Visuales:**

1. **LÃ­neas de conexiÃ³n**: Grosor = intensidad de atenciÃ³n
2. **Colores**: Escala de calor (azul = baja, rojo = alta atenciÃ³n)
3. **Tokens resaltados**: 
   - **Azul**: Token origen (desde donde se calcula atenciÃ³n)
   - **Rojo**: Token con mayor atenciÃ³n recibida
4. **Valores numÃ©ricos**: Puntuaciones exactas de atenciÃ³n

### **ğŸ” Patrones Comunes:**

- **AtenciÃ³n a sÃ­ mismo**: Tokens que se atienden a sÃ­ mismos
- **AtenciÃ³n posicional**: Preferencia por tokens cercanos
- **AtenciÃ³n sintÃ¡ctica**: Conexiones gramaticales (artÃ­culoâ†’sustantivo)
- **AtenciÃ³n semÃ¡ntica**: Relaciones de significado

## ğŸ“ Ejercicios Sugeridos

### **Nivel BÃ¡sico:**
1. Ejecuta el demo con la frase por defecto
2. Observa cÃ³mo cambian los patrones entre capas
3. Identifica quÃ© cabezas se especializan en quÃ© tipos de relaciones

### **Nivel Intermedio:**
1. Cambia la frase de entrada por una mÃ¡s compleja
2. Analiza cÃ³mo el modelo maneja ambigÃ¼edades
3. Compara patrones entre diferentes tipos de palabras (sustantivos vs verbos)

### **Nivel Avanzado:**
1. Modifica el cÃ³digo para usar otros modelos (GPT, RoBERTa)
2. Implementa mÃ©tricas de anÃ¡lisis de atenciÃ³n
3. Crea visualizaciones adicionales (mapas de calor, grafos)

## ğŸ”— Recursos Adicionales

### **ğŸ“š Lecturas Recomendadas:**
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Paper original de Transformers
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - ExplicaciÃ³n visual
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)

### **ğŸ¥ Videos Educativos:**
- [3Blue1Brown: Attention in transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- [Andrej Karpathy: Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)

### **ğŸ› ï¸ Herramientas Relacionadas:**
- [BertViz](https://github.com/jessevig/bertviz) - VisualizaciÃ³n avanzada de atenciÃ³n
- [Transformers Interpret](https://github.com/cdpierse/transformers-interpret) - Interpretabilidad
- [Attention Visualizer](https://github.com/PAIR-code/attention-viz) - Google Research

## ğŸš€ PrÃ³ximos Pasos

DespuÃ©s de completar este mÃ³dulo:

1. **MÃ³dulo 2**: Arquitectura completa de Transformers
2. **MÃ³dulo 3**: Modelos de lenguaje generativos (GPT)
3. **MÃ³dulo 4**: Fine-tuning y adaptaciÃ³n de modelos
4. **MÃ³dulo 5**: Llama 3 y modelos de cÃ³digo abierto

## ğŸ¤ Contribuciones

Â¿Encontraste un error o tienes una mejora? 

1. Abre un issue describiendo el problema
2. PropÃ³n cambios via pull request
3. Comparte tus experimentos y descubrimientos

## ğŸ“„ Licencia

Este material educativo estÃ¡ disponible bajo licencia MIT. SiÃ©ntete libre de usar, modificar y compartir.

---

**Â¡Disfruta explorando el fascinante mundo de la atenciÃ³n en los Transformers!** ğŸ§ âœ¨
