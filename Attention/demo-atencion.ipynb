{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.41.2 --quiet\n!pip install ipywidgets --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:33:12.861112Z","iopub.execute_input":"2025-08-07T16:33:12.861406Z","iopub.status.idle":"2025-08-07T16:33:36.540529Z","shell.execute_reply.started":"2025-08-07T16:33:12.861383Z","shell.execute_reply":"2025-08-07T16:33:36.538953Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as pe\n\nimport ipywidgets as widgets\nfrom transformers import AutoTokenizer, AutoModel\n\n# 1. Cargar modelo liviano y tokenizer\nMODEL_NAME = \"dccuchile/bert-base-spanish-wwm-cased\"  # Puedes cambiar a \"distilbert-base-multilingual-cased\" para español\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME, output_attentions=True)\n\n# 2. Frase de ejemplo\nsentence = \"El perro que persiguió al gato cruzó la calle.\"\n\n# 3. Tokenizar y obtener tensores\ninputs = tokenizer(sentence, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n    # Extrae las matrices de atención: (n_layers, batch, n_heads, seq_len, seq_len)\n    att_matrices = outputs.attentions\n\n# 4. Extraer tokens decodificados\ntokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n\n# 5. Widget para elegir capa, cabeza, y token origen\ndef plot_attention(layer=0, head=0, token_idx=0):\n    att = att_matrices[layer][0, head].cpu().numpy()  # (seq_len, seq_len)\n    plt.figure(figsize=(7, 5))\n    plt.barh(tokens, att[token_idx], color='lightblue')\n    plt.xlabel('Attention Score')\n    plt.title(f'Layer {layer}, Head {head}: attention from \"{tokens[token_idx]}\"')\n    plt.xlim(0, 1)\n    for i, v in enumerate(att[token_idx]):\n        plt.text(v + 0.01, i, f\"{v:.2f}\", va='center')\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    plt.show()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:33:43.325445Z","iopub.execute_input":"2025-08-07T16:33:43.325822Z","iopub.status.idle":"2025-08-07T16:34:00.164462Z","shell.execute_reply.started":"2025-08-07T16:33:43.325782Z","shell.execute_reply":"2025-08-07T16:34:00.163382Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e7366a846ad4bbdbdfdd845dce86c99"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc01f23c041f4d42941201ee1e7a9414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0590cbbaa2148e2be33fc58558ab978"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3519a8c1bbd8460a90c31958150555e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d5a4388b4c74022996e6ae75064ea58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc4893e14c6430aac519cedb3887ad8"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nBertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as pe\n\ndef plot_attention_lines_ultra_wide(layer=0, head=0, token_idx=0, thresh=0.02):\n    att = att_matrices[layer][0, head].cpu().numpy()\n    tokens_col = [t for t in tokens]\n    N = len(tokens_col)\n\n    # Aumenta mucho el ancho\n    fig, ax = plt.subplots(figsize=(15, max(7, N * 0.48)))\n    ax.axis('off')\n\n    y_pos = np.arange(N)[::-1]\n    norm = plt.Normalize(att[token_idx].min(), att[token_idx].max())\n    cmap = plt.get_cmap('coolwarm')\n\n    # Columna izquierda (tokens origen)\n    for i, token in enumerate(tokens_col):\n        color = '#2176ae' if i == token_idx else 'black'\n        fw = 'bold' if i == token_idx else 'normal'\n        bbox = dict(facecolor='#e3f3ff', edgecolor='#2176ae', boxstyle='round,pad=0.23') if i == token_idx else None\n        ax.text(0.08, y_pos[i]/N, token, fontsize=19, ha='right', va='center',\n                color=color, fontweight=fw, family='monospace', bbox=bbox,\n                path_effects=[pe.withStroke(linewidth=3, foreground='white')] if i==token_idx else None)\n\n    # Columna derecha (tokens destino)\n    for i, token in enumerate(tokens_col):\n        max_dest = np.argmax(att[token_idx])\n        fw = 'bold' if i == max_dest else 'normal'\n        bbox = dict(facecolor='#ffe6e6', edgecolor='#c44741', boxstyle='round,pad=0.23') if i == max_dest else None\n        ax.text(0.92, y_pos[i]/N, token, fontsize=19, ha='left', va='center',\n                color='black', fontweight=fw, family='monospace', bbox=bbox,\n                path_effects=[pe.withStroke(linewidth=3, foreground='white')] if i==max_dest else None)\n        # El valor numérico mucho más a la derecha\n        if att[token_idx][i] > thresh:\n            ax.text(1.38, y_pos[i]/N, f\"{att[token_idx][i]:.2f}\", fontsize=18, color='#c44741',\n                    ha='right', va='center', family='monospace', fontweight='bold' if i == max_dest else 'normal')\n\n    # Líneas de atención\n    for j in range(N):\n        score = att[token_idx][j]\n        if score > thresh:\n            line_color = cmap(norm(score))\n            linewidth = 1.3 + 7 * score\n            ax.plot([0.12, 0.91], [y_pos[token_idx]/N, y_pos[j]/N], color=line_color, alpha=0.88, linewidth=linewidth, solid_capstyle='round')\n\n    # Más margen derecho para no cortar el texto\n    ax.set_xlim(0, 1.45)\n    ax.set_ylim(-0.1, 1.1)\n    plt.title(f'Attention from {tokens_col[token_idx]}', fontsize=24, fontweight='bold', color='#2176ae', pad=20)\n    plt.figtext(1.32, 0.025, f\"Σ attention: {att[token_idx].sum():.2f}\", fontsize=13, color='gray', ha='right')\n    plt.tight_layout(rect=[0, 0.04, 1, 0.98])\n    plt.show()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:34:04.611789Z","iopub.execute_input":"2025-08-07T16:34:04.612346Z","iopub.status.idle":"2025-08-07T16:34:04.627905Z","shell.execute_reply.started":"2025-08-07T16:34:04.612317Z","shell.execute_reply":"2025-08-07T16:34:04.626832Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Widgets para seleccionar capa, cabeza, token\nlayer_selector = widgets.IntSlider(value=0, min=0, max=len(att_matrices)-1, step=1, description='Layer:')\nhead_selector = widgets.IntSlider(value=0, min=0, max=att_matrices[0].shape[1]-1, step=1, description='Head:')\ntoken_selector = widgets.IntSlider(value=1, min=0, max=len(tokens)-1, step=1, description='Token:')\n\nui = widgets.VBox([layer_selector, head_selector, token_selector])\nout = widgets.interactive_output(\n    plot_attention_lines_ultra_wide, {'layer': layer_selector, 'head': head_selector, 'token_idx': token_selector}\n)\n\ndisplay(ui, out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:34:08.050046Z","iopub.execute_input":"2025-08-07T16:34:08.050456Z","iopub.status.idle":"2025-08-07T16:34:08.499577Z","shell.execute_reply.started":"2025-08-07T16:34:08.050431Z","shell.execute_reply":"2025-08-07T16:34:08.498661Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(IntSlider(value=0, description='Layer:', max=11), IntSlider(value=0, description='Head:', max=1…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"307f724b49ac4d6e858fc1fd208cd60f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab10bc5aa9d543cb811c96fc37c36137"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}